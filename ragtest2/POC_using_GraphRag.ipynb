{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://microsoft.github.io/autogen/dev/reference/python/autogen_ext.tools.graphrag.html\n",
    "https://microsoft.github.io/graphrag/get_started/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating llm client with {'api_key': 'REDACTED,len=164', 'type': \"openai_chat\", 'encoding_model': 'cl100k_base', 'model': 'gpt-4o-mini', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'audience': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 50000, 'requests_per_minute': 1000, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25, 'responses': None}\n",
      "creating llm client with {'api_key': 'REDACTED,len=164', 'type': \"openai_chat\", 'encoding_model': 'cl100k_base', 'model': 'gpt-4o-mini', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'audience': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 50000, 'requests_per_minute': 1000, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25, 'responses': None}\n",
      "creating embedding llm client with {'api_key': 'REDACTED,len=164', 'type': \"openai_embedding\", 'encoding_model': 'cl100k_base', 'model': 'text-embedding-3-small', 'max_tokens': 4000, 'temperature': 0, 'top_p': 1, 'n': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'audience': None, 'deployment_name': None, 'model_supports_json': None, 'tokens_per_minute': 50000, 'requests_per_minute': 1000, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25, 'responses': None}\n"
     ]
    }
   ],
   "source": [
    "# %pip install python-dotenv autogen_ext autogen_agentchat\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import asyncio\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.tools.graphrag import GlobalSearchTool, LocalSearchTool\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "import gradio as gr\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "openai_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# Set up global search tool\n",
    "global_tool = GlobalSearchTool.from_settings(settings_path=\"./settings.yaml\")\n",
    "local_tool = LocalSearchTool.from_settings(settings_path=\"./settings.yaml\")\n",
    "\n",
    "# Create assistant agent with the global search tool\n",
    "assistant_agent = AssistantAgent(\n",
    "    name=\"search_assistant\",\n",
    "    tools=[global_tool,local_tool],\n",
    "    model_client=openai_client,\n",
    "    system_message=(\n",
    "        \"\"\"You are a tool selector AI assistant using the GraphRAG framework. \n",
    "        Your primary task is to determine the appropriate search tool to call based on the user's query. \n",
    "        For broader, abstract questions requiring a comprehensive understanding of the dataset, call the 'global_search' function.\n",
    "        For specific questions requiring a detailed, local search, call the 'local_search' function\"\"\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "def chat(question, history):\n",
    "    response =  asyncio.run(\n",
    "    assistant_agent.on_messages(\n",
    "        [TextMessage(content=question, source=\"user\")], CancellationToken()\n",
    "    ))\n",
    "    print(isinstance(response.chat_message.content, dict))\n",
    "    if isinstance(response.chat_message.content, dict):\n",
    "        return response.chat_message.content.get(\"answer\")\n",
    "    return response.chat_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: All map responses have score 0 (i.e., no relevant information found from the dataset), returning a canned 'I do not know' answer. You can try enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer =await chat('How many VMs are there in EAPSIPVXT9753 ?',[])\n",
    "\n",
    "# Run a sample query\n",
    "query = \"What is EAPSIPVXT9753?\"\n",
    "await Console(assistant_agent.run_stream(task=query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         source  \\\n",
      "0   AppBrewery Virtual Machines   \n",
      "1   AppBrewery Virtual Machines   \n",
      "2   AppBrewery Virtual Machines   \n",
      "3   AppBrewery Virtual Machines   \n",
      "4   AppBrewery Virtual Machines   \n",
      "..                          ...   \n",
      "95  AppBrewery Virtual Machines   \n",
      "96  AppBrewery Virtual Machines   \n",
      "97  AppBrewery Virtual Machines   \n",
      "98  AppBrewery Virtual Machines   \n",
      "99  AppBrewery Virtual Machines   \n",
      "\n",
      "                                                 text timestamp  \n",
      "0   VM: app96.kwbc.net, State: OFF, Status: Normal...  20250204  \n",
      "1   VM: test55.kwbc.net, State: OFF, Status: Norma...  20250204  \n",
      "2   VM: test48.kwbc.net, State: ON, Status: Specia...  20250204  \n",
      "3   VM: db60.kwbc.net, State: OFF, Status: Normal,...  20250204  \n",
      "4   VM: dev54.kwbc.net, State: OFF, Status: Normal...  20250204  \n",
      "..                                                ...       ...  \n",
      "95  VM: backup98.kwbc.net, State: OFF, Status: Nor...  20250204  \n",
      "96  VM: infra44.kwbc.net, State: ON, Status: Norma...  20250204  \n",
      "97  VM: test66.kwbc.net, State: OFF, Status: Norma...  20250204  \n",
      "98  VM: infra59.kwbc.net, State: OFF, Status: Norm...  20250204  \n",
      "99  VM: dev75.kwbc.net, State: OFF, Status: Specia...  20250204  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Collect records in a list\n",
    "records = []\n",
    "\n",
    "for index, row in vm_data_df.iterrows():\n",
    "    record = row.to_dict()\n",
    "    record_str = ', '.join(f\"{key}: '{value}'\" for key, value in record.items())\n",
    "    source = \"AppBrewery Virtual Machines\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "    records.append({\n",
    "        \"source\": source,\n",
    "        \"text\": record_str,\n",
    "        \"timestamp\": timestamp\n",
    "    })\n",
    "\n",
    "# Create a dataframe from the records\n",
    "records_df = pd.DataFrame(records)\n",
    "print(records_df)\n",
    "records_df.to_csv('records.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shivanksharma/Desktop/AI/Autogen0.4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
